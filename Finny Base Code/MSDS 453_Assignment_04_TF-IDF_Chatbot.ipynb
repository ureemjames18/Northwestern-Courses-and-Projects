{"cells":[{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1660768658575,"user":{"displayName":"Dimitry Kirtsman","userId":"11988189828040272864"},"user_tz":420},"id":"UwroEhsgVvpb"},"source":["# MSDS 453 Sentence based transformer chatbot"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10441,"status":"ok","timestamp":1701224209273,"user":{"displayName":"Ureem James","userId":"12473310198316461181"},"user_tz":300},"id":"jrSiD26qV3lK","outputId":"9633905d-2b4f-4535-d6a6-ab310e94b5ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import string\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import nltk\n","from collections import Counter\n","from dataclasses import dataclass\n","from timeit import default_timer as timer\n","import random\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","import gensim\n","from gensim.models import Word2Vec\n","\n","import spacy\n","from spacy import displacy\n","\n","from spacy.matcher import Matcher\n","from spacy.tokens import Span\n","\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","!pip install sentence_transformers\n","from sentence_transformers import SentenceTransformer, util\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","from IPython.display import display, HTML\n","\n","from typing import List, Callable, Dict, Tuple, Set\n","\n","pd.set_option('max_colwidth', 600)\n","pd.set_option('display.max_rows', 500)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701224209273,"user":{"displayName":"Ureem James","userId":"12473310198316461181"},"user_tz":300},"id":"lp6A50ADV3nS"},"outputs":[],"source":["#Load Sentence Transformer model optimized for  sentence cosine similarity calculations\n","\n","#The models below fully downloaded in Google Colab. This is the version of the google colab notebook but\n","#it was open in anoconda to be saved as pdf and the download graphics did not transfer properly so\n","#it seems like it didn't download. However, it did in the orignal google colab notebook, where all the\n","#analysis was run.\n","\n","# model1 = 'multi-qa-MiniLM-L6-cos-v1'\n","# model2 = 'all-MiniLM-L12-v2'\n","\n","# model = SentenceTransformer(model1)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1542,"status":"ok","timestamp":1701224210811,"user":{"displayName":"Ureem James","userId":"12473310198316461181"},"user_tz":300},"id":"d7YmRkBSV1Z6","outputId":"469a23fd-b549-4558-c895-6b4cbab0bf5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":254,"status":"ok","timestamp":1701224211062,"user":{"displayName":"Ureem James","userId":"12473310198316461181"},"user_tz":300},"id":"BJiRcLk9V1ci"},"outputs":[],"source":["# Only run this once, they will be downloaded.\n","nltk.download('stopwords',quiet=True)\n","nltk.download('wordnet',quiet=True)\n","nltk.download('punkt',quiet=True)\n","nltk.download('omw-1.4',quiet=True)\n","\n","\n","# # initializes an instance of NLTK's WordNet Lemmatizer\n","lemmer = nltk.stem.WordNetLemmatizer()\n","# # takes a list of tokens (words) as input and returns a list of their lemmas\n","\n","def LemTokens(tokens):\n","  return [lemmer.lemmatize(token) for token in tokens]\n","\n","\n","# # creates a dictionary that maps the Unicode code point of each punctuation character to None\n","# # used to remove punctuation from text\n","\n","\n","remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n","\n","\n","def LemNormalize(text):\n","# creates a set of stop words & punctuation characters\n","  stop = set(stopwords.words('english') + list(string.punctuation))\n","  # tokenizes the input text into words, converts them to lowercase, and removes the stop words and punctuation\n","  # remaining words are joined back into a single string\n","  text2= ' '.join([i for i in word_tokenize(text.lower()) if i not in stop])\n","  # string is then converted to lowercase, the punctuation is removed\n","  return LemTokens(nltk.word_tokenize(text2.lower().translate(remove_punct_dict)))"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701224211063,"user":{"displayName":"Ureem James","userId":"12473310198316461181"},"user_tz":300},"id":"z0o8BgwVCABM"},"outputs":[],"source":["#read in data\n","import pandas as pd\n","\n","CORPUS_PATH = '/content/gdrive/MyDrive/Data/student_loans-3.txt'\n","# CORPUS_PATH = '/content/gdrive/MyDrive/Data/student_loans-2.xlsx'\n","f=open(CORPUS_PATH,'r',errors = 'ignore')\n","# df = pd.read_excel(CORPUS_PATH)\n","\n","raw=f.read()\n","raw=raw.lower()# converts to lowercase\n","\n","#create list of sentences and words\n","sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences\n","word_tokens = nltk.word_tokenize(raw)# converts to list of words\n","\n","\n","# from_xlsx = df['Text'].tolist()\n","# for i in range(len(from_xlsx)):\n","#   from_xlsx[i] = from_xlsx[i].replace(\"\\n\", \" \")\n","# all_text = \" \".join(from_xlsx)\n","# raw = all_text.lower()\n","\n","# page_tokens = from_xlsx\n","\n","\n"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701224211063,"user":{"displayName":"Ureem James","userId":"12473310198316461181"},"user_tz":300},"id":"l7F3DUoM8nRp"},"outputs":[],"source":["#create greetings and greetings function\n","\n","GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n","GREETING_RESPONSES = [\"Hello\"]\n","\n","\n","# Checking for greetings\n","def greeting(sentence):\n","    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n","    for word in sentence.split():\n","        if word.lower() in GREETING_INPUTS:\n","            return random.choice(GREETING_RESPONSES)"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701224211063,"user":{"displayName":"Ureem James","userId":"12473310198316461181"},"user_tz":300},"id":"sD9KraOy8tC5"},"outputs":[],"source":["# Generating response function\n","# def response(user_response):\n","#     chatbot_response=''\n","#     sentence_encodings=model.encode(sent_tokens, convert_to_tensor=True)# generate sentence transformer embeddings\n","#     sentence_encodings=sentence_encodings.cpu()\n","#     vals = cosine_similarity(sentence_encodings[-1].reshape(1, -1), sentence_encodings) #the chatbot conversation code\n","#     #in the next cell adds the question as the last sentence of the sentence tokens, before calling this response function.\n","#     #The code takes the last sentence (which is the question) and gets cosine similarities vs all the sentences in the corpus,\n","#     #including itself\n","#     idx=vals.argsort()[0][-2] #gets the index of the second highest similarity (the first highest would be the question itself)\n","#     flat = vals.flatten()#reduces dimension of cosine similarity array to be able to sort\n","#     flat.sort() #sort the cosine similarity values\n","#     second_cos_sim_val = flat[-2] #get the second highest cosine similarity value.\n","#     if(second_cos_sim_val==0): #check the second highest cosine similarity value. If it's zero return the no match response,\n","#         #else return highest cosine similarity sentence.\n","#         chatbot_response=chatbot_response+\"Sorry, I do not have an answer to your question in my database. For more information, please visit https://studentaid.gov.\"\n","#         return chatbot_response\n","#     else:\n","#         chatbot_response = chatbot_response+sent_tokens[idx] #use index of highest cosine similarity to get original sentence\n","#         return chatbot_response\n","\n","def response(user_response):\n","  robo_response=''\n","  #initializetoemptyresponse\n","  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)\n","  tfidf = TfidfVec.fit_transform(sent_tokens)\n","  vals = cosine_similarity(tfidf[-1], tfidf)\n","  idx=vals.argsort()[0][-2]\n","  flat = vals.flatten()\n","  flat.sort()\n","  req_tfidf = flat[-2]\n","  if(req_tfidf==0):\n","    robo_response = robo_response+\"I am sorry! I don't understand you\"\n","    return robo_response\n","  else:\n","    robo_response = robo_response + sent_tokens[idx]\n","    return robo_response\n","\n","\n","\n","    # TfidfVec2 = TfidfVectorizer(tokenizer=LemNormalize)\n","    # specific_sent_tokens2 = nltk.sent_tokenize(page_tokens[idx])\n","    # specific_sent_tokens2.append(user_response)\n","    # tfidf2 = TfidfVec2.fit_transform(specific_sent_tokens2)\n","    # vals2 = cosine_similarity(tfidf2[-1], tfidf2)\n","    # idx2=vals2.argsort()[0][-2]\n","    # flat2 = vals2.flatten()\n","    # flat2.sort()\n","    # req_tfidf2 = flat2[-2]\n","    # if(req_tfidf2==0):\n","    #   robo_response=robo_response+\"I am sorry! I don't understand you\"\n","    #   return robo_response\n","    # robo_response = robo_response+specific_sent_tokens2[idx2]\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135018,"status":"ok","timestamp":1701224350030,"user":{"displayName":"Ureem James","userId":"12473310198316461181"},"user_tz":300},"id":"nUgzce_wVxXq","outputId":"4c57e044-a06d-42a2-9243-588bd0d0e2c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hi, my name is Finny! I aim to provide useful information regarding student loans and financial aid. To end session please type exit\n","\n","\n","what if I can't pay back my loans?\n","Answer: "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["once aggregate limits are met, the student is ineligible for additional stafford loans until they pay back a portion of the borrowed funds.\n","\n","\n","what are the consequences of defaulting on my loans?\n","Answer: and the consequences are much more severe.\n","\n","\n","loan default consequences are?\n","Answer: and the consequences are much more severe.\n","\n","\n","consequences of default\n","Answer: and the consequences are much more severe.\n","\n","\n","thanks\n","Answer: You are welcome - remember to consult your school counselor or visit https://studentaid.gov for more information.\n"]}],"source":["#Chatbot interaction code\n","\n","flag=True\n","print(\"Hi, my name is Finny! I aim to provide useful information regarding student loans and financial aid. To end session please type exit\")\n","print(\"\\n\")\n","\n","while(flag==True):\n","    user_response = input()\n","    user_response=user_response.lower()\n","    if user_response!='exit':\n","        if(user_response=='thanks' or user_response=='thank you' ):\n","            flag=False\n","            print(\"Answer: You are welcome - remember to consult your school counselor or visit https://studentaid.gov for more information.\")\n","        else:\n","            if(greeting(user_response)!=None):\n","                print(\"Answer: \"+greeting(user_response))\n","            else:\n","                sent_tokens.append(user_response)\n","                # page_tokens.append(user_response)\n","                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n","                final_words=list(set(word_tokens))\n","                print(\"Answer: \",end=\"\")\n","                print(response(user_response))\n","                print(\"\\n\")\n","                sent_tokens.remove(user_response)\n","                # page_tokens.remove(user_response)\n","    else:\n","        flag=False\n","        print(\"Goodbye and thank you for using the US Student Loans Information Chatbot. Please consult your school counselor or visit https://studentaid.gov for more information.\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}